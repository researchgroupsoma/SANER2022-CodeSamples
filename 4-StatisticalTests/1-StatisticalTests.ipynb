{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "judicial-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-outdoors",
   "metadata": {},
   "source": [
    "## Loading metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "north-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv(\"../1-GettingQuestions/sampleQuestions.csv\")\n",
    "samples[\"full_name\"] = samples[\"framework\"] + \"/\" +samples[\"path\"]\n",
    "samples = samples[\"full_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "perfect-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMetrics(listSamples):\n",
    "    metrics = dict()\n",
    "    for sample in listSamples:\n",
    "        metrics[sample] = pd.read_csv(f\"../3-DataMerge/{sample}.csv\", index_col=[0,1], na_filter=False)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "social-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_metrics_questions = loadMetrics(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-parker",
   "metadata": {},
   "source": [
    "## Mann Kendall Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-acoustic",
   "metadata": {},
   "source": [
    "Null Hypothesis: There is no monotonic trend.\n",
    "\n",
    "Alternative Hypothesis: There is a trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-intake",
   "metadata": {},
   "source": [
    "If p-value < 0.05 then null Hypothesis is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "utility-amazon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymannkendall in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from pymannkendall) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from pymannkendall) (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymannkendall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "collect-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymannkendall as mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "recent-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMannKendallTest(metrics, listSamples):\n",
    "    result = pd.DataFrame(index=listSamples)\n",
    "    result.index.rename(\"full_name\", inplace=True)\n",
    "    for sample in listSamples:\n",
    "        sampleMetric = metrics[sample]\n",
    "        for metric in sampleMetric.columns.values:\n",
    "            mkResult = mk.original_test(sampleMetric[metric])\n",
    "            result.loc[sample, metric+\" trend\"] = mkResult.trend\n",
    "            result.loc[sample, metric+\" pvalue\"] = mkResult.p\n",
    "            result.loc[sample, \"framework\"] = sample.split(\"/\")[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "durable-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mannKendallResult = extractMannKendallTest(normalized_metrics_questions, normalized_metrics_questions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "assisted-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "mannKendallResult.to_csv(\"mannKendallTest/mann_kendall_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-occasions",
   "metadata": {},
   "source": [
    "## Normality Test (Shapiro-Wilk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-server",
   "metadata": {},
   "source": [
    "Null Hyphotesis: The population has normal distribution\n",
    "\n",
    "Alternative Hyphotesis: The population has not normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-arcade",
   "metadata": {},
   "source": [
    "If p-value < 0.05 then null hyphotesis is rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-stupid",
   "metadata": {},
   "source": [
    "## Correlation Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-stevens",
   "metadata": {},
   "source": [
    "Null Hypothesis: there is no correlation between data\n",
    "\n",
    "Alternative Hypothesis: there is a correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-shock",
   "metadata": {},
   "source": [
    "If p-value < 0.05 then null Hypothesis is rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-revelation",
   "metadata": {},
   "source": [
    "If the metric is normalized we will apply the Pearson Test, otherwise we will apply Spearman Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "given-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "scheduled-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCorrelationTest(metrics, listSamples):\n",
    "    result = pd.DataFrame(index=listSamples)\n",
    "    result.index.rename(\"full_name\", inplace=True)\n",
    "    for sample in listSamples:\n",
    "        sampleMetric = metrics[sample]\n",
    "        for metric in sampleMetric.columns.values:\n",
    "            #test if the set has normal distribution\n",
    "            shapiroPvalue = stats.shapiro(sampleMetric[metric]).pvalue\n",
    "            if(shapiroPvalue < 0.05):\n",
    "                correlationResult = stats.spearmanr(sampleMetric[\"questions\"], sampleMetric[metric], nan_policy=\"omit\")    \n",
    "            else:\n",
    "                correlationResult = stats.pearsonr(sampleMetric[\"questions\"], sampleMetric[metric])\n",
    "            result.loc[sample, metric+\" correlation\"] = correlationResult[0]\n",
    "            result.loc[sample, metric+\" pvalue\"] = correlationResult[1]\n",
    "            result.loc[sample, \"framework\"] = sample.split(\"/\")[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "latest-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlationResult = extractCorrelationTest(normalized_metrics_questions, normalized_metrics_questions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "theoretical-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlationResult.to_csv(\"correlationTest/correlation_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
